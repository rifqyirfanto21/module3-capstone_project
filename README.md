# Module 3 Capstone Project

**Author**: Muhammad Rifqy Irfanto
**Program**: Purwadhika Data Engineering Bootcamp

## Project Overview
This repository contains the Module 3 capstone project: a small data platform demonstrating an end-to-end ETL and transformation workflow using Apache Airflow, dbt, PostgreSQL (local DWH), and Google BigQuery. The project includes:

- Data generation scripts to produce synthetic data and helper functions to insert into Postgres
- Airflow DAGs to initialize dimensional tables and load transactions into a PostgreSQL data warehouse
- Airflow DAGs to ingest data from PostgreSQL to BigQuery
- dbt project to transform and materialize marts from BigQuery
- Telegram notification callbacks integrated with Airflow for alerting
- Monitoring mechanism to track data consistency while being ingested from PostgreSQL to BigQuery

## Table of Contents
- [Project Overview](#project-overview)
- [Data Sources](#data-sources)
- [Database Schema](#database-schema)
- [Features](#features)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Docker Usage](#docker-usage)
- [Usage](#usage)
- [Airflow DAGs](#airflow-dags)
- [dbt](#dbt)
- [Configuration](#configuration)
- [Telegram Notifications](#telegram-notifications)
- [Technologies Used](#technologies-used)
- [Troubleshooting & Tips](#troubleshooting--tips)
- [Deliverables & Notes](#deliverables--notes)
- [Author & Acknowledgments](#author--acknowledgments)

## Data Sources
- Synthetic data generated by `scripts/generate_data.py`:
  - `users`, `payment_methods`, `shipping_methods`, `products`, `transactions`

> Note: There are no external raw CSV files in this project — all sample data is generated programmatically for testing and demo purposes.

## Database Schema
This project uses a simple star-like design for the local PostgreSQL data warehouse. The primary tables created/used by the scripts are:

### Dimension Tables
- `users` — user/master data
- `payment_methods` — supported payment methods
- `shipping_methods` — available shipping options
- `products` — product catalog

### Fact Table
- `transactions` — transactional records referencing dimension tables (`user_id`, `product_id`, `payment_method_id`, `shipping_method_id`)

The schema is initialized via `init.sql` (mounted to the `dwh_postgres` container) and populated by Airflow DAGs and `scripts/db_utils.py`.

## Features
- ETL orchestration using Airflow
- Automated dbt runs (models/test) triggered from Airflow
- Synthetic data generation with Faker
- SQLAlchemy-based insertion utilities for PostgreSQL
- Telegram alerts for DAG success/failure and data consistency (via Airflow Connections)
- Monitoring mechanism to check data rows consistency through each database

## Project Structure
```
module3-capstone_project/
├── airflow/
│   ├── dags/                      # Airflow DAGs
│   │   ├── dbt_dags.py            # marts transformation with dbt
│   │   ├── etl_bigquery_dags.py   # ingest tables from postgres to BigQuery
│   │   ├── etl_postgres_dags.py   # init dim tables & transactions to postgres
│   │   └── utils/                 # helper utilities (monitoring, notifications, db utils)
│   ├── logs/
│   └── plugins/
├── dbt_module3_capstone/          # dbt project (models, macros, profiles)
├── scripts/                       # data generation and DB helpers
│   ├── generate_data.py
│   ├── db_utils.py
│   └── main.py
├── config/
│   └── setting.py                 # DB connection for dwh
├── Dockerfile
├── docker-compose.yml
├── requirements.txt
├── requirements-airflow.txt
├── init.sql
├── init_metadata.sql
└── README.md
```

## Getting Started

### Prerequisites
- Docker & Docker Compose
- Python 3.11+
- Google Cloud credentials

### Installation (clone + venv)
1. Clone the repository:
```bash
git clone [your-repo-url]
cd module3-capstone_project
```

2. (Optional) Create a Python virtual environment for running local scripts:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

## Docker Usage
The project is designed to run inside Docker using the provided `docker-compose.yml`. The Airflow service builds a custom image using the provided `Dockerfile` which installs `requirements-airflow.txt`.

Start everything with:
```bash
docker compose up --build
```

Key service ports (default mapping):
- Airflow webserver: `http://localhost:8080` (user/password: `airflow`/`airflow` as per env)
- Postgres (Airflow metadata): `5441` -> container `5432`
- DWH Postgres: `5440` -> container `5432`

To stop and remove containers:
```bash
docker compose down
```

To remove volumes as well (clean start):
```bash
docker compose down -v
```

## Usage

### Initialize dimension tables (Airflow)
Use the DAG `init_dim_tables` (in `airflow/dags/etl_postgres_dags.py`) to create and populate dimension tables in the local DWH. This DAG uses PythonOperators that call `generate_data()` helpers and `db_utils.insert_*` functions.

### Load transactions (Airflow)
The `load_transactions` DAG generates and inserts transactional rows into the `transactions` table. It runs hourly by default.

### Tables ingestion (Airflow)
DAG `postgres_to_bigquery_init_dim_tables` and `postgres_to_bigquery_incremental_transactions` to ingest dimensional tables and transactions tables from postgres to BigQuery. It runs daily by default to ingest data from the day before.

### Run dbt models (Airflow)
The `dbt_run` DAG executes command to create marts table model from BigQuery table

These commands run inside the Airflow container (the DAG uses BashOperator: `cd /opt/dbt && dbt run --profiles-dir /opt/dbt`). Ensure your `profiles.yml` is configured correctly in `dbt_module3_capstone/profiles.yml` or mounted to `/opt/dbt`.

### Run scripts locally
You can generate and insert data locally without Airflow:
```bash
python scripts/main.py
```

This will generate sample users, products, and transactions and insert them into the Postgres DWH defined in `config/setting.py`.

## dbt
The dbt project lives in `dbt_module3_capstone/`. To run dbt locally or inside the container, ensure you have:

- Valid BigQuery credentials (if targeting BigQuery) and `GOOGLE_APPLICATION_CREDENTIALS` pointing to them inside the container
- Proper `profiles.yml` with dataset and project settings

Example (inside Docker/Airflow container):
```bash
cd /opt/dbt
dbt run --profiles-dir /opt/dbt
dbt test --profiles-dir /opt/dbt
```

## Configuration
- `config/setting.py` — DB connection settings for local scripts (used by `scripts/db_utils.py`). Default points to `dwh_postgres` container.
- `docker-compose.yml` — environment variables for Airflow and services. Important ones:
  - `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` — Airflow metadata DB
  - `GOOGLE_APPLICATION_CREDENTIALS` — path to GCP credentials
  - `DBT_PROFILES_DIR` — which folder contains dbt `profiles.yml` in the container

## Telegram Notifications
Notification utilities in `airflow/dags/utils/notif_callbacks.py` use an Airflow connection named `telegram_default`. Steps to configure:

1. Create Airflow connection `telegram_default` in the Airflow UI (Admin -> Connections). Paste the Bot Token in the `Password` field.
2. Update `TELEGRAM_CHAT_ID` inside `notif_callbacks.py` (or modify the code to read from Airflow Variables/Environment for better security).

Available functions:
- `consistent_notif_message(context, **kwargs)` — HTML-formatted consistent data notification
- `inconsistent_notif_message(context, **kwargs)` — HTML-formatted inconsistency alert
- `dag_success_notif_message(**context)` — DAG-level success notification
- `failed_notif_message(context)` — Failure notification with error details

## Technologies Used
- Python 3.11+
- Apache Airflow 2.10.2
- dbt (dbt-core, dbt-bigquery)
- PostgreSQL 14 (containers)
- Docker & Docker Compose
- Faker for synthetic data
- SQLAlchemy + psycopg2 for DB inserts

## Troubleshooting & Tips
- If your editor flags `airflow` or `requests` imports as unresolved, ensure your Python environment has the dependencies from `requirements-airflow.txt` or `requirements.txt` installed.
- If dbt commands fail inside the container, verify that `~/.config/gcloud` is mounted and `GOOGLE_APPLICATION_CREDENTIALS` points to the correct credentials file.
- If DB inserts fail, check `config/setting.py` for correct connection values and ensure `dwh_postgres` container is healthy.

## Deliverables & Notes
- The repo provides runnable Airflow DAGs and a dbt project for transforming data. Use Docker for an isolated reproducible environment.

### Video
- **Youtube Video Explanation:** [YouTube - Explanation](write link here)

### Warehouse Files
- **BigQuery Dataset:** [rifqy_computerstore_capstone3](https://console.cloud.google.com/bigquery?project=jcdeah-006&ws=!1m9!1m4!4m3!1sjcdeah-006!2srifqy_computerstore_capstone3!3smart_products_revenue!1m3!3m2!1sjcdeah-006!2srifqy_computerstore_capstone3)
---

## Author & Acknowledgments
**Author**: Muhammad Rifqy Irfanto

This project was completed as part of the Purwadhika Data Engineering Bootcamp Module 3 capstone requirement. Special thanks to the instructors and fellow cohort members for their guidance and collaboration.

---
*If you have any questions or feedback about this project, feel free to open an issue or reach out directly.*